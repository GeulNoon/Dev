{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPuORuVPTbJOggumh8V7iXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeulNoon/GeulNoon/blob/Start/KoBART/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2C2nRk7vZiIY",
        "outputId": "6e7d7800-e34f-4938-d390-27a4201f00cc"
      },
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobart\n",
            "  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-7y1rv6pv/kobart_8e9ef49e0a9e49a683890860ff8b72a8\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoBART /tmp/pip-install-7y1rv6pv/kobart_8e9ef49e0a9e49a683890860ff8b72a8\n",
            "Collecting transformers==4.3.3\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 14.9 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->kobart) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->kobart) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (4.8.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (4.62.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3->kobart) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.3->kobart) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (7.1.2)\n",
            "Building wheels for collected packages: kobart\n",
            "  Building wheel for kobart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobart: filename=kobart-0.4-py3-none-any.whl size=8543 sha256=dd7dc396b6a3fb44ecc11c9dbbc5d512c67b697095bb9e7002c9b8b592994e4b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a16yr7w6/wheels/6e/55/c4/bd4fede223bc304089ac8da2a2099a69db3fcd4b0e853383f5\n",
            "Successfully built kobart\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, torch, kobart\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed kobart-0.4 sacremoses-0.0.46 tokenizers-0.10.3 torch-1.7.1 transformers-4.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOsWUKSMaTCZ",
        "outputId": "f126a1c6-02ee-42f5-e831-b1cb0c9ec34d"
      },
      "source": [
        "!git clone https://github.com/seujung/KoBART-summarization.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'KoBART-summarization' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nwZ_OfniaYi6",
        "outputId": "a1eea3f6-8a8d-4e49-be5c-a99cbce46171"
      },
      "source": [
        "%cd /content/KoBART-summarization\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoBART-summarization\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.1.5)\n",
            "Collecting torch==1.10.0\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 80.2 MB/s eta 0:00:01tcmalloc: large alloc 1147494400 bytes == 0x560f69ec0000 @  0x7f7377434615 0x560f3080f4cc 0x560f308ef47a 0x560f308122ed 0x560f30903e1d 0x560f30885e99 0x560f308809ee 0x560f30813bda 0x560f30885d00 0x560f308809ee 0x560f30813bda 0x560f30882737 0x560f30904c66 0x560f30881daf 0x560f30904c66 0x560f30881daf 0x560f30904c66 0x560f30881daf 0x560f30814039 0x560f30857409 0x560f30812c52 0x560f30885c25 0x560f308809ee 0x560f30813bda 0x560f30882737 0x560f308809ee 0x560f30813bda 0x560f30881915 0x560f30813afa 0x560f30881c0d 0x560f308809ee\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 17 kB/s \n",
            "\u001b[?25hCollecting transformers==4.8.2\n",
            "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 34.8 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning==1.3.8\n",
            "  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n",
            "\u001b[K     |████████████████████████████████| 813 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting streamlit==1.1.0\n",
            "  Downloading streamlit-1.1.0-py2.py3-none-any.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Collecting pyDeprecate==0.3.0\n",
            "  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 51.0 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 51.0 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (2.7.0)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (3.17.3)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (5.1.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (0.10.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (1.5.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (4.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (3.0.0)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 67.3 MB/s \n",
            "\u001b[?25hCollecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit==1.1.0->-r requirements.txt (line 5)) (21.2.0)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (0.11.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 62.5 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.2->-r requirements.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit==1.1.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (7.6.5)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (5.1.1)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.6.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting ipython>=7.23.1\n",
            "  Downloading ipython-7.30.0-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 66.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (1.12.3)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (5.3.5)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.1.3)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8.2->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.23-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (3.5.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==1.1.0->-r requirements.txt (line 5)) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (4.9.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (5.6.1)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (2.0.8)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 50.7 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.1.0->-r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Building wheels for collected packages: future, blinker\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=2d70b8898611cabd34f44fce45b406d1088e8b758b03933e50c4fc6d9f700ec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=83237d2e89d1fb0f25644a54906e19b94c1c67ae57810852e1b3685e77e47a4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built future blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel, multidict, frozenlist, yarl, smmap, asynctest, async-timeout, aiosignal, torch, gitdb, fsspec, aiohttp, watchdog, validators, torchmetrics, pyyaml, pyDeprecate, pydeck, huggingface-hub, gitpython, future, blinker, base58, transformers, streamlit, pytorch-lightning\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.3.3\n",
            "    Uninstalling transformers-4.3.3:\n",
            "      Successfully uninstalled transformers-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kobart 0.4 requires torch==1.7.1, but you have torch 1.10.0 which is incompatible.\n",
            "kobart 0.4 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.23 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.6.0 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.0 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 base58-2.1.1 blinker-1.4 frozenlist-1.2.0 fsspec-2021.11.1 future-0.18.2 gitdb-4.0.9 gitpython-3.1.24 huggingface-hub-0.0.12 ipykernel-6.6.0 ipython-7.30.0 multidict-5.2.0 prompt-toolkit-3.0.23 pyDeprecate-0.3.0 pydeck-0.7.1 pytorch-lightning-1.3.8 pyyaml-5.4.1 smmap-5.0.0 streamlit-1.1.0 torch-1.10.0 torchmetrics-0.6.0 transformers-4.8.2 validators-0.18.2 watchdog-2.1.6 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit",
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADDCmiqga2CI",
        "outputId": "a2f827de-205d-4b0c-ba21-9e2c862e77c9"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 36.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0\n",
            "    Uninstalling torch-1.10.0:\n",
            "      Successfully uninstalled torch-1.10.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1+cu101 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1+cu101 which is incompatible.\n",
            "kobart 0.4 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75GdgQHTbI6z",
        "outputId": "caa7cfa6-e325-4c11-bbaa-6e156c5c237c"
      },
      "source": [
        "!tar -zxvf /content/KoBART-summarization/data/train.tar.gz\n",
        "!tar -zxvf /content/KoBART-summarization/data/test.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.tsv\n",
            "test.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuL6pwKRbKzb"
      },
      "source": [
        "!mv /content/train.tsv /content/KoBART-summarization/data\n",
        "!mv /content/test.tsv /content/KoBART-summarization/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvQo3ycxbOCA",
        "outputId": "d14d3642-93a8-4a79-a657-99b297a0bc7f"
      },
      "source": [
        "%cd /content/KoBART-summarization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoBART-summarization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiRI_3Zre4qV",
        "outputId": "5c84cc0f-11fe-489f-885c-5606bb58df1f"
      },
      "source": [
        "!pip install gdown\n",
        "!python download_binary.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Download config.json\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1H13loH6dS_2c2Z21kaBtgz42QsjkdAwO\n",
            "To: /content/KoBART-summarization/kobart_summary/config.json\n",
            "100% 1.20k/1.20k [00:00<00:00, 1.83MB/s]\n",
            "Download pytorch_model.bin\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D7BAXK_0faWW39c0ptE3FtROsVRbTNwI\n",
            "To: /content/KoBART-summarization/kobart_summary/pytorch_model.bin\n",
            "100% 496M/496M [00:02<00:00, 172MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21wBmnW0bQZb",
        "outputId": "115c53de-4147-4920-fe8f-28fd4a17b9ac"
      },
      "source": [
        "!pip install torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "  Downloading torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.10.0.2)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "Successfully installed torchtext-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR52FSR6bXme",
        "outputId": "a11f4097-d3c1-4c08-ad23-9e801a3adce9"
      },
      "source": [
        "!python train.py  --gradient_clip_val 1.0 --max_epochs 3 --default_root_dir logs  --gpus 1 --batch_size 4 --num_workers 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=4, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='logs', deterministic=False, distributed_backend=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_algorithm='norm', gradient_clip_val=1.0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=3, max_len=512, max_steps=None, max_time=None, min_epochs=None, min_steps=None, model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='data/test.tsv', tpu_cores=None, track_grad_norm=-1, train_file='data/train.tsv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:root:number of workers 2, data length 34242\n",
            "INFO:root:num_train_steps : 12840\n",
            "INFO:root:num_warmup_steps : 1284\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "495.440   Total estimated model params size (MB)\n",
            "Validation sanity check: 0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n",
            "Epoch 0:  80% 8580/10702 [55:47<13:47,  2.56it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/2141 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 8600/10702 [55:50<13:38,  2.57it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  81% 8620/10702 [55:52<13:29,  2.57it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  81% 8640/10702 [55:54<13:20,  2.58it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  81% 8660/10702 [55:57<13:11,  2.58it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  81% 8680/10702 [55:59<13:02,  2.58it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  81% 8700/10702 [56:01<12:53,  2.59it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  81% 8720/10702 [56:03<12:44,  2.59it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  82% 8740/10702 [56:06<12:35,  2.60it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  82% 8760/10702 [56:08<12:26,  2.60it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  82% 8780/10702 [56:10<12:17,  2.60it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  82% 8800/10702 [56:13<12:09,  2.61it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  82% 8820/10702 [56:15<12:00,  2.61it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  83% 8840/10702 [56:17<11:51,  2.62it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  83% 8860/10702 [56:19<11:42,  2.62it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  83% 8880/10702 [56:22<11:33,  2.63it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  83% 8900/10702 [56:24<11:25,  2.63it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  83% 8920/10702 [56:26<11:16,  2.63it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  84% 8940/10702 [56:29<11:07,  2.64it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  84% 8960/10702 [56:31<10:59,  2.64it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  84% 8980/10702 [56:33<10:50,  2.65it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  84% 9000/10702 [56:35<10:42,  2.65it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  84% 9020/10702 [56:38<10:33,  2.65it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  84% 9040/10702 [56:40<10:25,  2.66it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  85% 9060/10702 [56:42<10:16,  2.66it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  85% 9080/10702 [56:45<10:08,  2.67it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  85% 9100/10702 [56:47<09:59,  2.67it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  85% 9120/10702 [56:49<09:51,  2.67it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  85% 9140/10702 [56:52<09:43,  2.68it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  86% 9160/10702 [56:54<09:34,  2.68it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  86% 9180/10702 [56:56<09:26,  2.69it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  86% 9200/10702 [56:58<09:18,  2.69it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  86% 9220/10702 [57:01<09:09,  2.69it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  86% 9240/10702 [57:03<09:01,  2.70it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  87% 9260/10702 [57:05<08:53,  2.70it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  87% 9280/10702 [57:08<08:45,  2.71it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  87% 9300/10702 [57:10<08:37,  2.71it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  87% 9320/10702 [57:12<08:29,  2.72it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  87% 9340/10702 [57:14<08:20,  2.72it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  87% 9360/10702 [57:17<08:12,  2.72it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  88% 9380/10702 [57:19<08:04,  2.73it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  88% 9400/10702 [57:21<07:56,  2.73it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  88% 9420/10702 [57:24<07:48,  2.74it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  88% 9440/10702 [57:26<07:40,  2.74it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  88% 9460/10702 [57:28<07:32,  2.74it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  89% 9480/10702 [57:30<07:24,  2.75it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  89% 9500/10702 [57:33<07:16,  2.75it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  89% 9520/10702 [57:35<07:09,  2.76it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  89% 9540/10702 [57:37<07:01,  2.76it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  89% 9560/10702 [57:40<06:53,  2.76it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  90% 9580/10702 [57:42<06:45,  2.77it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  90% 9600/10702 [57:44<06:37,  2.77it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  90% 9620/10702 [57:46<06:29,  2.77it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  90% 9640/10702 [57:49<06:22,  2.78it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  90% 9660/10702 [57:51<06:14,  2.78it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  90% 9680/10702 [57:53<06:06,  2.79it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  91% 9700/10702 [57:56<05:59,  2.79it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  91% 9720/10702 [57:58<05:51,  2.79it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  91% 9740/10702 [58:00<05:43,  2.80it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  91% 9760/10702 [58:03<05:36,  2.80it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  91% 9780/10702 [58:05<05:28,  2.81it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  92% 9800/10702 [58:07<05:20,  2.81it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  92% 9820/10702 [58:09<05:13,  2.81it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  92% 9840/10702 [58:12<05:05,  2.82it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  92% 9860/10702 [58:14<04:58,  2.82it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  92% 9880/10702 [58:16<04:50,  2.83it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  93% 9900/10702 [58:19<04:43,  2.83it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  93% 9920/10702 [58:21<04:36,  2.83it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  93% 9940/10702 [58:23<04:28,  2.84it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  93% 9960/10702 [58:25<04:21,  2.84it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  93% 9980/10702 [58:28<04:13,  2.84it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  93% 10000/10702 [58:30<04:06,  2.85it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  94% 10020/10702 [58:32<03:59,  2.85it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  94% 10040/10702 [58:35<03:51,  2.86it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  94% 10060/10702 [58:37<03:44,  2.86it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  94% 10080/10702 [58:39<03:37,  2.86it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  94% 10100/10702 [58:41<03:29,  2.87it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  95% 10120/10702 [58:44<03:22,  2.87it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  95% 10140/10702 [58:46<03:15,  2.88it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  95% 10160/10702 [58:48<03:08,  2.88it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  95% 10180/10702 [58:51<03:01,  2.88it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  95% 10200/10702 [58:53<02:53,  2.89it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  95% 10220/10702 [58:55<02:46,  2.89it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  96% 10240/10702 [58:57<02:39,  2.89it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  96% 10260/10702 [59:00<02:32,  2.90it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  96% 10280/10702 [59:02<02:25,  2.90it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  96% 10300/10702 [59:04<02:18,  2.91it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  96% 10320/10702 [59:07<02:11,  2.91it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  97% 10340/10702 [59:09<02:04,  2.91it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  97% 10360/10702 [59:11<01:57,  2.92it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  97% 10380/10702 [59:13<01:50,  2.92it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  97% 10400/10702 [59:16<01:43,  2.92it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  97% 10420/10702 [59:18<01:36,  2.93it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  98% 10440/10702 [59:20<01:29,  2.93it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  98% 10460/10702 [59:23<01:22,  2.94it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  98% 10480/10702 [59:25<01:15,  2.94it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  98% 10500/10702 [59:27<01:08,  2.94it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  98% 10520/10702 [59:29<01:01,  2.95it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  98% 10540/10702 [59:32<00:54,  2.95it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  99% 10560/10702 [59:34<00:48,  2.95it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  99% 10580/10702 [59:36<00:41,  2.96it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  99% 10600/10702 [59:39<00:34,  2.96it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  99% 10620/10702 [59:41<00:27,  2.97it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0:  99% 10640/10702 [59:43<00:20,  2.97it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0: 100% 10660/10702 [59:46<00:14,  2.97it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0: 100% 10680/10702 [59:48<00:07,  2.98it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0: 100% 10700/10702 [59:50<00:00,  2.98it/s, loss=1.39, v_num=1, val_loss=9.440, train_loss=1.040]\n",
            "Epoch 0: 100% 10702/10702 [59:53<00:00,  2.98it/s, loss=1.37, v_num=1, val_loss=1.320, train_loss=1.850]\n",
            "                                                   \u001b[AEpoch 0, global step 8560: val_loss reached 1.32208 (best 1.32208), saving model to \"/content/KoBART-summarization/logs/model_chp/epoch=00-val_loss=1.322.ckpt\" as top 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "tcmalloc: large alloc 1074479104 bytes == 0x560d89cd4000 @  0x7fa5d85af615 0x560c97b944cc 0x560c97c7447a 0x560c97b9af0c 0x7fa5ad637934 0x7fa5ad639704 0x7fa5ad609430 0x7fa59df4f465 0x7fa59df4b9ca 0x7fa59df50609 0x7fa5ad60cdcb 0x7fa5ad2920a0 0x560c97b98098 0x560c97c0b4d9 0x560c97c05ced 0x560c97b98bda 0x560c97c06915 0x560c97c059ee 0x560c97b98bda 0x560c97c0ad00 0x560c97b98afa 0x560c97c06915 0x560c97b98afa 0x560c97c06c0d 0x560c97b98afa 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda\n",
            "tcmalloc: large alloc 1343102976 bytes == 0x560d22978000 @  0x7fa5d85af615 0x560c97b944cc 0x560c97c7447a 0x560c97b9af0c 0x7fa5ad637934 0x7fa5ad639704 0x7fa5ad609430 0x7fa59df4f465 0x7fa59df4b9ca 0x7fa59df50609 0x7fa5ad60cdcb 0x7fa5ad2920a0 0x560c97b98098 0x560c97c0b4d9 0x560c97c05ced 0x560c97b98bda 0x560c97c06915 0x560c97c059ee 0x560c97b98bda 0x560c97c0ad00 0x560c97b98afa 0x560c97c06915 0x560c97b98afa 0x560c97c06c0d 0x560c97b98afa 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda\n",
            "tcmalloc: large alloc 1678884864 bytes == 0x560d72a5a000 @  0x7fa5d85af615 0x560c97b944cc 0x560c97c7447a 0x560c97b9af0c 0x7fa5ad637934 0x7fa5ad639704 0x7fa5ad609430 0x7fa59df4f465 0x7fa59df4b9ca 0x7fa59df50609 0x7fa5ad60cdcb 0x7fa5ad2920a0 0x560c97b98098 0x560c97c0b4d9 0x560c97c05ced 0x560c97b98bda 0x560c97c06915 0x560c97c059ee 0x560c97b98bda 0x560c97c0ad00 0x560c97b98afa 0x560c97c06915 0x560c97b98afa 0x560c97c06c0d 0x560c97b98afa 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda\n",
            "tcmalloc: large alloc 1678884864 bytes == 0x560d22978000 @  0x7fa5d85af615 0x560c97b944cc 0x560c97c7447a 0x560c97b9af0c 0x7fa5ad637934 0x7fa5ad639704 0x7fa5ad609430 0x7fa59df4f465 0x7fa59df4b9ca 0x7fa59df50609 0x7fa5ad60cdcb 0x7fa5ad2920a0 0x560c97b98098 0x560c97c0b4d9 0x560c97c05ced 0x560c97b98bda 0x560c97c06915 0x560c97c059ee 0x560c97b98bda 0x560c97c0ad00 0x560c97b98afa 0x560c97c06915 0x560c97b98afa 0x560c97c06c0d 0x560c97b98afa 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda 0x560c97c06c0d 0x560c97c059ee 0x560c97b98bda\n",
            "Epoch 1:  80% 8580/10702 [55:26<13:42,  2.58it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/2141 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 8600/10702 [55:28<13:33,  2.58it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  81% 8620/10702 [55:30<13:24,  2.59it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  81% 8640/10702 [55:33<13:15,  2.59it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  81% 8660/10702 [55:35<13:06,  2.60it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  81% 8680/10702 [55:37<12:57,  2.60it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  81% 8700/10702 [55:40<12:48,  2.60it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  81% 8720/10702 [55:42<12:39,  2.61it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  82% 8740/10702 [55:44<12:30,  2.61it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  82% 8760/10702 [55:46<12:21,  2.62it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  82% 8780/10702 [55:49<12:13,  2.62it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  82% 8800/10702 [55:51<12:04,  2.63it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  82% 8820/10702 [55:53<11:55,  2.63it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  83% 8840/10702 [55:56<11:46,  2.63it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  83% 8860/10702 [55:58<11:38,  2.64it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  83% 8880/10702 [56:00<11:29,  2.64it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  83% 8900/10702 [56:03<11:20,  2.65it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  83% 8920/10702 [56:05<11:12,  2.65it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  84% 8940/10702 [56:07<11:03,  2.65it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  84% 8960/10702 [56:09<10:55,  2.66it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  84% 8980/10702 [56:12<10:46,  2.66it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  84% 9000/10702 [56:14<10:38,  2.67it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  84% 9020/10702 [56:16<10:29,  2.67it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  84% 9040/10702 [56:19<10:21,  2.68it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  85% 9060/10702 [56:21<10:12,  2.68it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  85% 9080/10702 [56:23<10:04,  2.68it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  85% 9100/10702 [56:25<09:56,  2.69it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  85% 9120/10702 [56:28<09:47,  2.69it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  85% 9140/10702 [56:30<09:39,  2.70it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  86% 9160/10702 [56:32<09:31,  2.70it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  86% 9180/10702 [56:35<09:22,  2.70it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  86% 9200/10702 [56:37<09:14,  2.71it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  86% 9220/10702 [56:39<09:06,  2.71it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  86% 9240/10702 [56:41<08:58,  2.72it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  87% 9260/10702 [56:44<08:50,  2.72it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  87% 9280/10702 [56:46<08:41,  2.72it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  87% 9300/10702 [56:48<08:33,  2.73it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  87% 9320/10702 [56:51<08:25,  2.73it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  87% 9340/10702 [56:53<08:17,  2.74it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  87% 9360/10702 [56:55<08:09,  2.74it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  88% 9380/10702 [56:57<08:01,  2.74it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  88% 9400/10702 [57:00<07:53,  2.75it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  88% 9420/10702 [57:02<07:45,  2.75it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  88% 9440/10702 [57:04<07:37,  2.76it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  88% 9460/10702 [57:07<07:29,  2.76it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  89% 9480/10702 [57:09<07:22,  2.76it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  89% 9500/10702 [57:11<07:14,  2.77it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  89% 9520/10702 [57:14<07:06,  2.77it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  89% 9540/10702 [57:16<06:58,  2.78it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  89% 9560/10702 [57:18<06:50,  2.78it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  90% 9580/10702 [57:20<06:42,  2.78it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  90% 9600/10702 [57:23<06:35,  2.79it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  90% 9620/10702 [57:25<06:27,  2.79it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  90% 9640/10702 [57:27<06:19,  2.80it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  90% 9660/10702 [57:30<06:12,  2.80it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  90% 9680/10702 [57:32<06:04,  2.80it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  91% 9700/10702 [57:34<05:56,  2.81it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  91% 9720/10702 [57:36<05:49,  2.81it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  91% 9740/10702 [57:39<05:41,  2.82it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  91% 9760/10702 [57:41<05:34,  2.82it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  91% 9780/10702 [57:43<05:26,  2.82it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  92% 9800/10702 [57:46<05:19,  2.83it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  92% 9820/10702 [57:48<05:11,  2.83it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  92% 9840/10702 [57:50<05:04,  2.84it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  92% 9860/10702 [57:52<04:56,  2.84it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  92% 9880/10702 [57:55<04:49,  2.84it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  93% 9900/10702 [57:57<04:41,  2.85it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  93% 9920/10702 [57:59<04:34,  2.85it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  93% 9940/10702 [58:02<04:26,  2.85it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  93% 9960/10702 [58:04<04:19,  2.86it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  93% 9980/10702 [58:06<04:12,  2.86it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  93% 10000/10702 [58:08<04:04,  2.87it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  94% 10020/10702 [58:11<03:57,  2.87it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  94% 10040/10702 [58:13<03:50,  2.87it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  94% 10060/10702 [58:15<03:43,  2.88it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  94% 10080/10702 [58:18<03:35,  2.88it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  94% 10100/10702 [58:20<03:28,  2.89it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  95% 10120/10702 [58:22<03:21,  2.89it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  95% 10140/10702 [58:24<03:14,  2.89it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  95% 10160/10702 [58:27<03:07,  2.90it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  95% 10180/10702 [58:29<02:59,  2.90it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  95% 10200/10702 [58:31<02:52,  2.90it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  95% 10220/10702 [58:34<02:45,  2.91it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  96% 10240/10702 [58:36<02:38,  2.91it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  96% 10260/10702 [58:38<02:31,  2.92it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  96% 10280/10702 [58:41<02:24,  2.92it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  96% 10300/10702 [58:43<02:17,  2.92it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  96% 10320/10702 [58:45<02:10,  2.93it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  97% 10340/10702 [58:47<02:03,  2.93it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  97% 10360/10702 [58:50<01:56,  2.93it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  97% 10380/10702 [58:52<01:49,  2.94it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  97% 10400/10702 [58:54<01:42,  2.94it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  97% 10420/10702 [58:57<01:35,  2.95it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  98% 10440/10702 [58:59<01:28,  2.95it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  98% 10460/10702 [59:01<01:21,  2.95it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  98% 10480/10702 [59:03<01:15,  2.96it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  98% 10500/10702 [59:06<01:08,  2.96it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  98% 10520/10702 [59:08<01:01,  2.96it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  98% 10540/10702 [59:10<00:54,  2.97it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  99% 10560/10702 [59:13<00:47,  2.97it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  99% 10580/10702 [59:15<00:40,  2.98it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  99% 10600/10702 [59:17<00:34,  2.98it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  99% 10620/10702 [59:19<00:27,  2.98it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1:  99% 10640/10702 [59:22<00:20,  2.99it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1: 100% 10660/10702 [59:24<00:14,  2.99it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1: 100% 10680/10702 [59:26<00:07,  2.99it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1: 100% 10700/10702 [59:29<00:00,  3.00it/s, loss=1.15, v_num=1, val_loss=1.320, train_loss=1.070]\n",
            "Epoch 1: 100% 10702/10702 [59:31<00:00,  3.00it/s, loss=1.11, v_num=1, val_loss=1.320, train_loss=0.801]\n",
            "                                                   \u001b[AEpoch 1, global step 17121: val_loss reached 1.32071 (best 1.32071), saving model to \"/content/KoBART-summarization/logs/model_chp/epoch=01-val_loss=1.321.ckpt\" as top 2\n",
            "Epoch 2:  80% 8580/10702 [55:43<13:46,  2.57it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/2141 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 8600/10702 [55:46<13:37,  2.57it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  81% 8620/10702 [55:48<13:28,  2.57it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  81% 8640/10702 [55:50<13:19,  2.58it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  81% 8660/10702 [55:53<13:10,  2.58it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  81% 8680/10702 [55:55<13:01,  2.59it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  81% 8700/10702 [55:57<12:52,  2.59it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  81% 8720/10702 [56:00<12:43,  2.60it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  82% 8740/10702 [56:02<12:34,  2.60it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  82% 8760/10702 [56:04<12:25,  2.60it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  82% 8780/10702 [56:06<12:17,  2.61it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  82% 8800/10702 [56:09<12:08,  2.61it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  82% 8820/10702 [56:11<11:59,  2.62it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  83% 8840/10702 [56:13<11:50,  2.62it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  83% 8860/10702 [56:16<11:41,  2.62it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  83% 8880/10702 [56:18<11:33,  2.63it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  83% 8900/10702 [56:20<11:24,  2.63it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  83% 8920/10702 [56:22<11:15,  2.64it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  84% 8940/10702 [56:25<11:07,  2.64it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  84% 8960/10702 [56:27<10:58,  2.65it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  84% 8980/10702 [56:29<10:50,  2.65it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  84% 9000/10702 [56:32<10:41,  2.65it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  84% 9020/10702 [56:34<10:32,  2.66it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  84% 9040/10702 [56:36<10:24,  2.66it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  85% 9060/10702 [56:38<10:16,  2.67it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  85% 9080/10702 [56:41<10:07,  2.67it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  85% 9100/10702 [56:43<09:59,  2.67it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  85% 9120/10702 [56:45<09:50,  2.68it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  85% 9140/10702 [56:48<09:42,  2.68it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  86% 9160/10702 [56:50<09:34,  2.69it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  86% 9180/10702 [56:52<09:25,  2.69it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  86% 9200/10702 [56:55<09:17,  2.69it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  86% 9220/10702 [56:57<09:09,  2.70it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  86% 9240/10702 [56:59<09:01,  2.70it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  87% 9260/10702 [57:01<08:52,  2.71it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  87% 9280/10702 [57:04<08:44,  2.71it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  87% 9300/10702 [57:06<08:36,  2.71it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  87% 9320/10702 [57:08<08:28,  2.72it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  87% 9340/10702 [57:11<08:20,  2.72it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  87% 9360/10702 [57:13<08:12,  2.73it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  88% 9380/10702 [57:15<08:04,  2.73it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  88% 9400/10702 [57:17<07:56,  2.73it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  88% 9420/10702 [57:20<07:48,  2.74it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  88% 9440/10702 [57:22<07:40,  2.74it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  88% 9460/10702 [57:24<07:32,  2.75it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  89% 9480/10702 [57:27<07:24,  2.75it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  89% 9500/10702 [57:29<07:16,  2.75it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  89% 9520/10702 [57:31<07:08,  2.76it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  89% 9540/10702 [57:33<07:00,  2.76it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  89% 9560/10702 [57:36<06:52,  2.77it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  90% 9580/10702 [57:38<06:45,  2.77it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  90% 9600/10702 [57:40<06:37,  2.77it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  90% 9620/10702 [57:43<06:29,  2.78it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  90% 9640/10702 [57:45<06:21,  2.78it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  90% 9660/10702 [57:47<06:14,  2.79it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  90% 9680/10702 [57:50<06:06,  2.79it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  91% 9700/10702 [57:52<05:58,  2.79it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  91% 9720/10702 [57:54<05:51,  2.80it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  91% 9740/10702 [57:56<05:43,  2.80it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  91% 9760/10702 [57:59<05:35,  2.81it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  91% 9780/10702 [58:01<05:28,  2.81it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  92% 9800/10702 [58:03<05:20,  2.81it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  92% 9820/10702 [58:06<05:13,  2.82it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  92% 9840/10702 [58:08<05:05,  2.82it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  92% 9860/10702 [58:10<04:58,  2.82it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  92% 9880/10702 [58:12<04:50,  2.83it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  93% 9900/10702 [58:15<04:43,  2.83it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  93% 9920/10702 [58:17<04:35,  2.84it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  93% 9940/10702 [58:19<04:28,  2.84it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  93% 9960/10702 [58:22<04:20,  2.84it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  93% 9980/10702 [58:24<04:13,  2.85it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  93% 10000/10702 [58:26<04:06,  2.85it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  94% 10020/10702 [58:29<03:58,  2.86it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  94% 10040/10702 [58:31<03:51,  2.86it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  94% 10060/10702 [58:33<03:44,  2.86it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  94% 10080/10702 [58:35<03:36,  2.87it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  94% 10100/10702 [58:38<03:29,  2.87it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  95% 10120/10702 [58:40<03:22,  2.87it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  95% 10140/10702 [58:42<03:15,  2.88it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  95% 10160/10702 [58:45<03:08,  2.88it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  95% 10180/10702 [58:47<03:00,  2.89it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  95% 10200/10702 [58:49<02:53,  2.89it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  95% 10220/10702 [58:51<02:46,  2.89it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  96% 10240/10702 [58:54<02:39,  2.90it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  96% 10260/10702 [58:56<02:32,  2.90it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  96% 10280/10702 [58:58<02:25,  2.90it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  96% 10300/10702 [59:01<02:18,  2.91it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  96% 10320/10702 [59:03<02:11,  2.91it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  97% 10340/10702 [59:05<02:04,  2.92it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  97% 10360/10702 [59:07<01:57,  2.92it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  97% 10380/10702 [59:10<01:50,  2.92it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  97% 10400/10702 [59:12<01:43,  2.93it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  97% 10420/10702 [59:14<01:36,  2.93it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  98% 10440/10702 [59:17<01:29,  2.93it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  98% 10460/10702 [59:19<01:22,  2.94it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  98% 10480/10702 [59:21<01:15,  2.94it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  98% 10500/10702 [59:23<01:08,  2.95it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  98% 10520/10702 [59:26<01:01,  2.95it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  98% 10540/10702 [59:28<00:54,  2.95it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  99% 10560/10702 [59:30<00:48,  2.96it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  99% 10580/10702 [59:33<00:41,  2.96it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  99% 10600/10702 [59:35<00:34,  2.96it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  99% 10620/10702 [59:37<00:27,  2.97it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2:  99% 10640/10702 [59:40<00:20,  2.97it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2: 100% 10660/10702 [59:42<00:14,  2.98it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2: 100% 10680/10702 [59:44<00:07,  2.98it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2: 100% 10700/10702 [59:46<00:00,  2.98it/s, loss=1.19, v_num=1, val_loss=1.320, train_loss=1.360]\n",
            "Epoch 2: 100% 10702/10702 [59:49<00:00,  2.98it/s, loss=1.15, v_num=1, val_loss=1.370, train_loss=0.601]\n",
            "                                                   \u001b[AEpoch 2, global step 25682: val_loss reached 1.37135 (best 1.32071), saving model to \"/content/KoBART-summarization/logs/model_chp/epoch=02-val_loss=1.371.ckpt\" as top 3\n",
            "Epoch 2: 100% 10702/10702 [1:00:05<00:00,  2.97it/s, loss=1.15, v_num=1, val_loss=1.370, train_loss=0.601]Saving latest checkpoint...\n",
            "Epoch 2: 100% 10702/10702 [1:00:12<00:00,  2.96it/s, loss=1.15, v_num=1, val_loss=1.370, train_loss=0.601]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pylicSd5pW0r",
        "outputId": "b9301b63-70dc-4a9c-bd13-2e78f552cb13"
      },
      "source": [
        "!python get_model_binary.py --hparams logs/tb_logs/default/version_1/hparams.yaml --model_binary logs/model_chp/epoch=02-val_loss=1.371.ckpt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_model_binary.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  hparams = yaml.load(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRox2RL-xEGD",
        "outputId": "36d81548-9630-4382-a82a-b4015f419072"
      },
      "source": [
        "!streamlit run infer.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIqn1PpfpeNR",
        "outputId": "b2d37ce4-7247-40e9-9523-c570cb0e0b37"
      },
      "source": [
        "import torch\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained('./kobart_summary')\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "\n",
        "text = \"\"\n",
        "f = open(\"./text.txt\", \"r\", encoding = \"utf-8\")\n",
        "#encoding = \"utf-8\"을 붙이지 않으면 에러가 일어날 수 있으므로 주의!\n",
        "while True:\n",
        "   line = f.readline()\n",
        "   if not line: break\n",
        "   text += line\n",
        "f.close()\n",
        "if text:\n",
        "  text = text.replace('\\n', '')\n",
        "  input_ids = tokenizer.encode(text)\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "  output = model.generate(input_ids, eos_token_id=1, max_length=512, num_beams=5)\n",
        "  output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하루아침에 인위적으로 만들어지지 않으며 추진 과정에서 위험이 수반되기도 하는 창조 도시를 만들기 위해서는 도시 고유의 특성을 면밀히 고찰하여 창조 산업, 창조 계층, 창조 환경의 역동성을 최대화할 수 있는 조건이 무엇인지 밝혀낼 필요가 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrleD_kcflce"
      },
      "source": [
        "!TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD=2000000000"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}